# çº¿æ€§æ¨¡å‹ç¬”è®°

## 1ã€å‘é‡åŒ–

åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡å‘é‡åŒ–æ¥é«˜æ•ˆè®¡ç®—é¢„æµ‹å€¼ï¼š

ä¼ ç»Ÿå…¬å¼ï¼ˆå•ä¸ªæ ·æœ¬ï¼‰ï¼š

$$
\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$$

å‘é‡è¡¨ç¤ºï¼š

$$
\hat{y} = \theta^T x
$$

> é€šå¸¸æˆ‘ä»¬åœ¨è¾“å…¥ $x$ ä¸­æ·»åŠ ä¸€ä¸ª $x_0 = 1$ï¼Œæ¥ç»Ÿä¸€è¡¨ç¤ºåç½®é¡¹ã€‚

---

## 2ã€çº¿æ€§å›å½’æ¨¡å‹å…¬å¼

å‡è®¾æœ‰ $m$ ä¸ªæ ·æœ¬ã€$n$ ä¸ªç‰¹å¾ï¼š

- ç‰¹å¾çŸ©é˜µï¼š$X \in \mathbb{R}^{m \times n}$
- å‚æ•°å‘é‡ï¼š$\theta \in \mathbb{R}^{n \times 1}$
- æ ‡ç­¾å‘é‡ï¼š$y \in \mathbb{R}^{m \times 1}$

æ¨¡å‹é¢„æµ‹å…¬å¼ï¼š

$$
\hat{y} = X \theta
$$

---

## 3ã€æŸå¤±å‡½æ•°ï¼ˆä»£ä»·å‡½æ•°ï¼‰

ä½¿ç”¨ **å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰** ä½œä¸ºæŸå¤±å‡½æ•°ï¼š

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2
$$

å‘é‡åŒ–è¡¨è¾¾å¼ï¼š

$$
J(\theta) = \frac{1}{2m} (X\theta - y)^T (X\theta - y)
$$

---

## 4ã€æ¢¯åº¦ä¸‹é™æ³•

ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•° $\theta$ï¼š

$$
\theta := \theta - \alpha \cdot \frac{1}{m} X^T (X\theta - y)
$$

å…¶ä¸­ï¼š
- $\alpha$ æ˜¯å­¦ä¹ ç‡
- $X^T$ æ˜¯ç‰¹å¾çŸ©é˜µçš„è½¬ç½®
- $(X\theta - y)$ æ˜¯é¢„æµ‹è¯¯å·®

---

## 5ã€Python å®ç°ç¤ºä¾‹

```python
import numpy as np

def computerCost(X,y,theta):
    inner=np.power(((X*theta.T)-y),2)
    return np.sum(inner)/(2*len(X))

def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)
    
    for i in range(iters):
        error = (X * theta.T) - y
        
        for j in range(parameters):
            term = np.multiply(error, X[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))
            
        theta = temp
        cost[i] = computerCost(X, y, theta)
        
    return theta, cost

```
# 6ã€ä½¿ç”¨ sklearn å®ç°çº¿æ€§å›å½’æ¨¡å‹

`scikit-learn` æ˜¯ Python ä¸­æœ€å¸¸ç”¨çš„æœºå™¨å­¦ä¹ åº“ï¼Œä½¿ç”¨å®ƒå¯ä»¥éå¸¸æ–¹ä¾¿åœ°å®ç°çº¿æ€§å›å½’ã€‚

### âœ… åŸºæœ¬æ­¥éª¤å¦‚ä¸‹ï¼š

1. å¯¼å…¥æ¨¡å‹ç±» `LinearRegression`
2. æ‹†åˆ†ç‰¹å¾å’Œæ ‡ç­¾
3. æ‹Ÿåˆæ¨¡å‹
4. æŸ¥çœ‹å‚æ•° / è¿›è¡Œé¢„æµ‹ / è¯„ä¼°æ¨¡å‹

### ğŸ“¦ ç¤ºä¾‹ä»£ç ï¼š
å‡è®¾æœ‰ä¸€ä¸ªex1data1.txtæ–‡ä»¶ï¼Œé‡Œé¢åŒ…å«äº†æˆ¿å±‹çš„é¢ç§¯ï¼Œå§å®¤æ•°

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
# è§£å†³pltä¸­æ–‡ä¹±ç é—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False    # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
#åŠ è½½æ•°æ®
df=pd.read_csv("C:\\Users\\s1597\\Desktop\\Python\\Machine Learning\\Linear_regression\\ex1data2.txt")

#åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
X=df.iloc[:,0:2].values # ç‰¹å¾ï¼šå‰ä¸¤åˆ—
y=df.iloc[:,2].values # æ ‡ç­¾ï¼šç¬¬ä¸‰åˆ—

#df.iloc[:,0:2]è¡¨ç¤ºå–å‡ºå‰ä¸¤åˆ—æ•°æ®ï¼Œdf.iloc[:,2]è¡¨ç¤ºå–å‡ºç¬¬ä¸‰åˆ—æ•°æ®,
# ilocæ˜¯pandasä¸­ç”¨äºæŒ‰ä½ç½®ç´¢å¼•çš„å‡½æ•°ï¼Œ:è¡¨ç¤ºå–æ‰€æœ‰è¡Œï¼Œ
# 0:2è¡¨ç¤ºå–ç¬¬0åˆ—åˆ°ç¬¬2åˆ—ï¼ˆä¸åŒ…æ‹¬ç¬¬2åˆ—ï¼‰ï¼Œ2è¡¨ç¤ºå–ç¬¬2åˆ—æ•°æ®

#åˆ›å»ºæ¨¡å‹
model=LinearRegression()
model.fit(X,y)

#è¾“å‡ºæ¨¡å‹å‚æ•°
print("æˆªè·:", model.intercept_)
print("ç³»æ•°:", model.coef_)

#è¿›è¡Œé¢„æµ‹
y_pred=model.predict(X)

# é¢„æµ‹å€¼ä¸å®é™…å€¼å¯¹æ¯”æ•£ç‚¹å›¾
plt.figure(figsize=(8, 6))
plt.scatter(y, y_pred, color='blue', label='é¢„æµ‹ vs å®é™…')
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--', label='ç†æƒ³é¢„æµ‹çº¿')
plt.xlabel('å®é™…æˆ¿ä»·')
plt.ylabel('é¢„æµ‹æˆ¿ä»·')
plt.title('çº¿æ€§å›å½’é¢„æµ‹æ•ˆæœå¯¹æ¯”å›¾')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```
## 7ã€numpyä¸­çš„åˆ‡ç‰‡
### X[n,:]æ˜¯å–ç¬¬1ç»´ä¸­ä¸‹æ ‡ä¸ºnçš„å…ƒç´ çš„æ‰€æœ‰å€¼
### X[1,:]å³å–ç¬¬ä¸€ç»´ä¸­ä¸‹æ ‡ä¸º1çš„å…ƒç´ çš„æ‰€æœ‰å€¼
### X[:,0]å°±æ˜¯å–æ‰€æœ‰è¡Œçš„ç¬¬0ä¸ªæ•°æ®,
### X[:,1] å°±æ˜¯å–æ‰€æœ‰è¡Œçš„ç¬¬1ä¸ªæ•°æ®
### X[:, m:n]ï¼Œå³å–æ‰€æœ‰æ•°æ®çš„ç¬¬måˆ°n-1åˆ—æ•°æ®ï¼Œå«å·¦ä¸å«å³

## 8ã€ç‰¹å¾ç¼©æ”¾ï¼ˆFeature Scalingï¼‰

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œä¸åŒç‰¹å¾çš„æ•°å€¼èŒƒå›´å·®å¼‚è¿‡å¤§ï¼ˆå¦‚é¢ç§¯ vs å§å®¤æ•°é‡ï¼‰æ—¶ï¼Œå®¹æ˜“å¯¼è‡´æ¨¡å‹è®­ç»ƒç¼“æ…¢æˆ–æ”¶æ•›ä¸ç¨³å®šã€‚

### âœ… è§£å†³æ–¹æ³•ï¼šæ ‡å‡†åŒ–ï¼ˆZ-score Normalizationï¼‰

å°†æ‰€æœ‰ç‰¹å¾ç¼©æ”¾ä¸ºå‡å€¼ä¸º 0ã€æ ‡å‡†å·®ä¸º 1 çš„æ•°æ®ï¼š

$$
x' = \frac{x - \mu}{\sigma}
$$

å…¶ä¸­ï¼š
- $x$ æ˜¯åŸå§‹å€¼
- $\mu$ æ˜¯è¯¥ç‰¹å¾çš„å‡å€¼
- $\sigma$ æ˜¯è¯¥ç‰¹å¾çš„æ ‡å‡†å·®

## ğŸ“¦ sklearn å®ç°ï¼š

```python
from sklearn.preprocessing import StandardScaler

# åˆå§‹åŒ–ç¼©æ”¾å™¨
scaler = StandardScaler()

# å¯¹ç‰¹å¾è¿›è¡Œç¼©æ”¾
X_scaled = scaler.fit_transform(X)

# ä½¿ç”¨ç¼©æ”¾åçš„æ•°æ®è®­ç»ƒæ¨¡å‹
model_scaled = LinearRegression()
model_scaled.fit(X_scaled, y)

# é¢„æµ‹ & è¯„ä¼°
y_pred_scaled = model_scaled.predict(X_scaled)
print("ç¼©æ”¾å MSE:", mean_squared_error(y, y_pred_scaled))

```
## 8ã€ä½¿ç”¨é¢ç§¯å’Œå§å®¤æ•°é‡çš„å¤šé¡¹å¼å›å½’

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **æˆ¿å±‹é¢ç§¯** å’Œ **å§å®¤æ•°é‡** ä½œä¸ºç‰¹å¾ï¼Œæ„é€ ä¸€ä¸ª **å¤šé¡¹å¼å›å½’æ¨¡å‹**ï¼Œä»¥æ•æ‰æ›´å¤æ‚çš„æˆ¿ä»·è¶‹åŠ¿ã€‚

### âœ… ç›®æ ‡ï¼š

- ä½¿ç”¨ **æˆ¿å±‹é¢ç§¯** å’Œ **å§å®¤æ•°é‡** ä½œä¸ºè¾“å…¥ç‰¹å¾ï¼Œæ„å»ºä¸€ä¸ªå¤šé¡¹å¼å›å½’æ¨¡å‹ã€‚
- ä½¿ç”¨ `PolynomialFeatures` ç±»æ¥æ‰©å±•ç‰¹å¾ï¼Œå¹¶åŠ å…¥æ›´é«˜æ¬¡æ–¹çš„ç‰¹å¾ã€‚

### ğŸ“¦ ä»£ç å®ç°ï¼š

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# è§£å†³pltä¸­æ–‡ä¹±ç é—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False    # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
#åŠ è½½æ•°æ®
df=pd.read_csv("C:\\Users\\s1597\\Desktop\\Python\\Machine Learning\\Linear_regression\\ex1data2.txt")



# å‡è®¾æˆ‘ä»¬å·²ç»åŠ è½½äº†æ•°æ®å¹¶æå–äº†ç‰¹å¾ X å’Œç›®æ ‡å€¼ y
# X åŒ…å«é¢ç§¯å’Œå§å®¤æ•°é‡ç‰¹å¾
X = df.iloc[:, 0:2].values  # å–é¢ç§¯å’Œå§å®¤æ•°é‡ä½œä¸ºç‰¹å¾
y = df.iloc[:, 2].values    # æˆ¿ä»·ä½œä¸ºç›®æ ‡å€¼

# 1. åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾ï¼ˆdegree=2 ä»£è¡¨æ„é€  x, x^2, x1*x2 ç­‰äº¤äº’é¡¹ï¼‰
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# 2. ä½¿ç”¨å¤šé¡¹å¼ç‰¹å¾è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
model_poly = LinearRegression()
model_poly.fit(X_poly, y)

# 3. è¿›è¡Œé¢„æµ‹
y_poly_pred = model_poly.predict(X_poly)

# 4. å¯è§†åŒ–æ‹Ÿåˆæ•ˆæœ
# å¯¹äºäºŒç»´æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ç»˜åˆ¶ä¸€ä¸ªä¸‰ç»´å›¾ï¼Œå±•ç¤ºä¸åŒæˆ¿å±‹é¢ç§¯å’Œå§å®¤æ•°é‡ä¸‹çš„æˆ¿ä»·
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# ç»˜åˆ¶å®é™…æ•°æ®ç‚¹
ax.scatter(X[:, 0], X[:, 1], y, color='blue', label='å®é™…æ•°æ®')

# åˆ›å»ºç½‘æ ¼æ•°æ®
x_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 30)
y_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 30)

#np.linspace()å‡½æ•°ç”¨äºç”ŸæˆæŒ‡å®šèŒƒå›´å†…çš„ç­‰é—´è·æ•°å€¼,é‡Œé¢çš„å‚æ•°æœ‰ä¸‰ä¸ªï¼Œåˆ†åˆ«æ˜¯èµ·å§‹å€¼ã€ç»ˆæ­¢å€¼å’Œæ­¥é•¿ï¼Œè¿™é‡Œçš„æ­¥é•¿æ˜¯30ï¼Œèµ·å§‹å€¼æ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾çš„æœ€å°å€¼ï¼Œç»ˆæ­¢å€¼æ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾çš„æœ€å¤§å€¼
#np.meshgrid()å‡½æ•°ç”¨äºç”Ÿæˆç½‘æ ¼ç‚¹åæ ‡çŸ©é˜µ,é‡Œé¢çš„å‚æ•°æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯xå’Œyçš„èŒƒå›´
#np.column_stack()å‡½æ•°ç”¨äºå°†å¤šä¸ªæ•°ç»„æŒ‰åˆ—åˆå¹¶æˆä¸€ä¸ªæ–°çš„æ•°ç»„,é‡Œé¢çš„å‚æ•°æ˜¯è¦åˆå¹¶çš„æ•°ç»„

x_grid, y_grid = np.meshgrid(x_range, y_range)
grid_points = np.column_stack([x_grid.ravel(), y_grid.ravel()])

# å¯¹ç½‘æ ¼ç‚¹è¿›è¡Œé¢„æµ‹
#poly.transform()å‡½æ•°ç”¨äºå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºå¤šé¡¹å¼ç‰¹å¾,é‡Œé¢çš„å‚æ•°æ˜¯è¦è½¬æ¢çš„æ•°æ®
#model_poly.predict()å‡½æ•°ç”¨äºå¯¹å¤šé¡¹å¼ç‰¹å¾è¿›è¡Œé¢„æµ‹,é‡Œé¢çš„å‚æ•°æ˜¯å¤šé¡¹å¼ç‰¹å¾æ•°æ®
grid_poly = poly.transform(grid_points)
z_grid = model_poly.predict(grid_poly)

# ç»˜åˆ¶å›å½’é¢
ax.plot_surface(x_grid, y_grid, z_grid.reshape(x_grid.shape), color='green', alpha=0.5, label='æ‹Ÿåˆé¢')
#plot_surface()å‡½æ•°ç”¨äºç»˜åˆ¶ä¸‰ç»´æ›²é¢å›¾,é‡Œé¢çš„å‚æ•°æœ‰ä¸‰ä¸ªï¼Œåˆ†åˆ«æ˜¯xã€yå’Œzçš„åæ ‡

ax.set_xlabel('æˆ¿å±‹é¢ç§¯')
ax.set_ylabel('å§å®¤æ•°é‡')
ax.set_zlabel('æˆ¿ä»·')
ax.set_title('å¤šé¡¹å¼å›å½’ï¼šé¢ç§¯å’Œå§å®¤æ•°é‡å¯¹æˆ¿ä»·çš„å½±å“')
ax.legend()

plt.tight_layout()
plt.show()

```
